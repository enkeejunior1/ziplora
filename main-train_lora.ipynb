{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import (\n",
    "    DistributedDataParallelKwargs,\n",
    "    ProjectConfiguration,\n",
    "    set_seed,\n",
    ")\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from huggingface_hub.utils import insecure_hashlib\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from PIL.ImageOps import exif_transpose\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "import diffusers\n",
    "from diffusers import (\n",
    "    AutoPipelineForText2Image,\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    StableDiffusionXLPipeline,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.loaders import LoraLoaderMixin\n",
    "from diffusers.models.lora import LoRALinearLayer\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import compute_snr, unet_lora_state_dict\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, output_dir, instance_dir, prompt, valid_prompt):\n",
    "        self.pretrained_model_name_or_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "        self.output_dir = output_dir\n",
    "        self.instance_dir = instance_dir\n",
    "        self.prompt = prompt\n",
    "        self.validation_prompt = valid_prompt\n",
    "        \n",
    "        self.rank = 64\n",
    "        self.resolution = 1024\n",
    "        self.train_batch_size = 1\n",
    "        self.learning_rate = 5e-5\n",
    "        self.report_to = \"wandb\"\n",
    "        self.lr_scheduler = \"constant\"\n",
    "        self.lr_warmup_steps = 0\n",
    "        self.max_train_steps = 1000\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        \n",
    "        self.validation_epochs = 50\n",
    "        self.seed = 42\n",
    "        self.mixed_precision = \"fp16\"\n",
    "\n",
    "        # self.enable_xformers_memory_efficient_attention = True\n",
    "        self.gradient_checkpointing  = True\n",
    "        # self.use_8bit_adam  = True\n",
    "        # self.push_to_hub  = True\n",
    "        self.train_text_encoder = False\n",
    "\n",
    "        self.logging_dir = 'logs'\n",
    "\n",
    "args = Args(\n",
    "    output_dir=\"lora-sdxl-dog\",\n",
    "    instance_dir=\"dog\",\n",
    "    prompt=\"a sbu dog\",\n",
    "    valid_prompt=\"a sbu dog in a bucket\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.local/lib/python3.10/site-packages/accelerate/accelerator.py:387: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "04/03/2024 05:53:12 - WARNING - accelerate.utils.other - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "04/03/2024 05:53:12 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...:  14%|â–ˆâ–        | 1/7 [00:00<00:00,  9.58it/s]{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...:  29%|â–ˆâ–ˆâ–Š       | 2/7 [00:15<00:45,  9.09s/it]{'rescale_betas_zero_snr', 'sigma_max', 'sigma_min', 'timestep_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 4/7 [00:16<00:11,  3.68s/it]Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5/7 [00:18<00:06,  3.16s/it]Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6/7 [00:23<00:03,  3.77s/it]Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:23<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# preparation #\n",
    "###############\n",
    "\n",
    "# set seed\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# logging\n",
    "logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "if not is_wandb_available():\n",
    "    pass\n",
    "else:\n",
    "    import wandb\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "# accelerator setting\n",
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=args.output_dir, logging_dir=logging_dir\n",
    ")\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    mixed_precision=args.mixed_precision,\n",
    "    log_with=args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    "    kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)],\n",
    ")\n",
    "\n",
    "# accelerator logging\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "#  path\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "# TODO: implement with_prior_preservation\n",
    "\n",
    "#########\n",
    "# train #\n",
    "#########\n",
    "# dtype\n",
    "if accelerator.mixed_precision == \"fp16\":\n",
    "    weight_dtype = torch.float16\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    weight_dtype = torch.bfloat16\n",
    "if accelerator.mixed_precision == \"fp32\":\n",
    "    weight_dtype = torch.float32\n",
    "\n",
    "# Load scheduler and models\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(args.pretrained_model_name_or_path, torch_dtype=weight_dtype)\n",
    "vae = pipe.vae.to(accelerator.device, dtype=torch.float32)\n",
    "unet = pipe.unet.to(accelerator.device)\n",
    "text_encoder_one = pipe.text_encoder.to(accelerator.device)\n",
    "text_encoder_two = pipe.text_encoder_2.to(accelerator.device)\n",
    "text_tokenizer_one = pipe.tokenizer\n",
    "text_tokenizer_two = pipe.tokenizer_2\n",
    "scheduler = pipe.scheduler\n",
    "\n",
    "# We only train the additional adapter LoRA layers\n",
    "vae.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "text_encoder_two.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "# gradient ckpt\n",
    "if args.gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    if args.train_text_encoder:\n",
    "        text_encoder_one.gradient_checkpointing_enable()\n",
    "        text_encoder_two.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'set_lora_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     attn_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(attn_module, n)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Set the `lora_layer` attribute of the attention-related matrices.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mattn_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_lora_layer\u001b[49m(\n\u001b[1;32m     12\u001b[0m     LoRALinearLayer(\n\u001b[1;32m     13\u001b[0m         in_features\u001b[38;5;241m=\u001b[39mattn_module\u001b[38;5;241m.\u001b[39mto_q\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m     14\u001b[0m         out_features\u001b[38;5;241m=\u001b[39mattn_module\u001b[38;5;241m.\u001b[39mto_q\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m     15\u001b[0m         rank\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mrank,\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m attn_module\u001b[38;5;241m.\u001b[39mto_k\u001b[38;5;241m.\u001b[39mset_lora_layer(\n\u001b[1;32m     19\u001b[0m     LoRALinearLayer(\n\u001b[1;32m     20\u001b[0m         in_features\u001b[38;5;241m=\u001b[39mattn_module\u001b[38;5;241m.\u001b[39mto_k\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m attn_module\u001b[38;5;241m.\u001b[39mto_v\u001b[38;5;241m.\u001b[39mset_lora_layer(\n\u001b[1;32m     26\u001b[0m     LoRALinearLayer(\n\u001b[1;32m     27\u001b[0m         in_features\u001b[38;5;241m=\u001b[39mattn_module\u001b[38;5;241m.\u001b[39mto_v\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1687\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1687\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'set_lora_layer'"
     ]
    }
   ],
   "source": [
    "# now we will add new LoRA weights to the attention layers\n",
    "# Set correct lora layers\n",
    "unet_lora_parameters = []\n",
    "for attn_processor_name, _ in unet.attn_processors.items():\n",
    "    # Parse the attention module.\n",
    "    attn_module = unet\n",
    "    for n in attn_processor_name.split(\".\")[:-1]:\n",
    "        attn_module = getattr(attn_module, n)\n",
    "\n",
    "    # Set the `lora_layer` attribute of the attention-related matrices.\n",
    "    attn_module.to_q.set_lora_layer(\n",
    "        LoRALinearLayer(\n",
    "            in_features=attn_module.to_q.in_features,\n",
    "            out_features=attn_module.to_q.out_features,\n",
    "            rank=args.rank,\n",
    "        )\n",
    "    )\n",
    "    attn_module.to_k.set_lora_layer(\n",
    "        LoRALinearLayer(\n",
    "            in_features=attn_module.to_k.in_features,\n",
    "            out_features=attn_module.to_k.out_features,\n",
    "            rank=args.rank,\n",
    "        )\n",
    "    )\n",
    "    attn_module.to_v.set_lora_layer(\n",
    "        LoRALinearLayer(\n",
    "            in_features=attn_module.to_v.in_features,\n",
    "            out_features=attn_module.to_v.out_features,\n",
    "            rank=args.rank,\n",
    "        )\n",
    "    )\n",
    "    attn_module.to_out[0].set_lora_layer(\n",
    "        LoRALinearLayer(\n",
    "            in_features=attn_module.to_out[0].in_features,\n",
    "            out_features=attn_module.to_out[0].out_features,\n",
    "            rank=args.rank,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Accumulate the LoRA params to optimize.\n",
    "    unet_lora_parameters.extend(attn_module.to_q.lora_layer.parameters())\n",
    "    unet_lora_parameters.extend(attn_module.to_k.lora_layer.parameters())\n",
    "    unet_lora_parameters.extend(attn_module.to_v.lora_layer.parameters())\n",
    "    unet_lora_parameters.extend(attn_module.to_out[0].lora_layer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    \n",
    "\n",
    "    # The text encoder comes from ðŸ¤— transformers, so we cannot directly modify it.\n",
    "    # So, instead, we monkey-patch the forward calls of its attention-blocks.\n",
    "    if args.train_text_encoder:\n",
    "        # ensure that dtype is float32, even if rest of the model that isn't trained is loaded in fp16\n",
    "        text_lora_parameters_one = LoraLoaderMixin._modify_text_encoder(\n",
    "            text_encoder_one, dtype=torch.float32, rank=args.rank\n",
    "        )\n",
    "        text_lora_parameters_two = LoraLoaderMixin._modify_text_encoder(\n",
    "            text_encoder_two, dtype=torch.float32, rank=args.rank\n",
    "        )\n",
    "\n",
    "    # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format\n",
    "    def save_model_hook(models, weights, output_dir):\n",
    "        if accelerator.is_main_process:\n",
    "            # there are only two options here. Either are just the unet attn processor layers\n",
    "            # or there are the unet and text encoder atten layers\n",
    "            unet_lora_layers_to_save = None\n",
    "            text_encoder_one_lora_layers_to_save = None\n",
    "            text_encoder_two_lora_layers_to_save = None\n",
    "\n",
    "            for model in models:\n",
    "                if isinstance(model, type(accelerator.unwrap_model(unet))):\n",
    "                    unet_lora_layers_to_save = unet_lora_state_dict(model)\n",
    "                elif isinstance(\n",
    "                    model, type(accelerator.unwrap_model(text_encoder_one))\n",
    "                ):\n",
    "                    text_encoder_one_lora_layers_to_save = text_encoder_lora_state_dict(\n",
    "                        model\n",
    "                    )\n",
    "                elif isinstance(\n",
    "                    model, type(accelerator.unwrap_model(text_encoder_two))\n",
    "                ):\n",
    "                    text_encoder_two_lora_layers_to_save = text_encoder_lora_state_dict(\n",
    "                        model\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "                # make sure to pop weight so that corresponding model is not saved again\n",
    "                weights.pop()\n",
    "\n",
    "            StableDiffusionXLPipeline.save_lora_weights(\n",
    "                output_dir,\n",
    "                unet_lora_layers=unet_lora_layers_to_save,\n",
    "                text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "                text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,\n",
    "            )\n",
    "\n",
    "    def load_model_hook(models, input_dir):\n",
    "        unet_ = None\n",
    "        text_encoder_one_ = None\n",
    "        text_encoder_two_ = None\n",
    "\n",
    "        while len(models) > 0:\n",
    "            model = models.pop()\n",
    "\n",
    "            if isinstance(model, type(accelerator.unwrap_model(unet))):\n",
    "                unet_ = model\n",
    "            elif isinstance(model, type(accelerator.unwrap_model(text_encoder_one))):\n",
    "                text_encoder_one_ = model\n",
    "            elif isinstance(model, type(accelerator.unwrap_model(text_encoder_two))):\n",
    "                text_encoder_two_ = model\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "        lora_state_dict, network_alphas = LoraLoaderMixin.lora_state_dict(input_dir)\n",
    "        LoraLoaderMixin.load_lora_into_unet(\n",
    "            lora_state_dict, network_alphas=network_alphas, unet=unet_\n",
    "        )\n",
    "\n",
    "        text_encoder_state_dict = {\n",
    "            k: v for k, v in lora_state_dict.items() if \"text_encoder.\" in k\n",
    "        }\n",
    "        LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "            text_encoder_state_dict,\n",
    "            network_alphas=network_alphas,\n",
    "            text_encoder=text_encoder_one_,\n",
    "        )\n",
    "\n",
    "        text_encoder_2_state_dict = {\n",
    "            k: v for k, v in lora_state_dict.items() if \"text_encoder_2.\" in k\n",
    "        }\n",
    "        LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "            text_encoder_2_state_dict,\n",
    "            network_alphas=network_alphas,\n",
    "            text_encoder=text_encoder_two_,\n",
    "        )\n",
    "\n",
    "    accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "    accelerator.register_load_state_pre_hook(load_model_hook)\n",
    "\n",
    "    # Enable TF32 for faster training on Ampere GPUs,\n",
    "    # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "    if args.allow_tf32:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate\n",
    "            * args.gradient_accumulation_steps\n",
    "            * args.train_batch_size\n",
    "            * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Optimization parameters\n",
    "    unet_lora_parameters_with_lr = {\n",
    "        \"params\": unet_lora_parameters,\n",
    "        \"lr\": args.learning_rate,\n",
    "    }\n",
    "    if args.train_text_encoder:\n",
    "        # different learning rate for text encoder and unet\n",
    "        text_lora_parameters_one_with_lr = {\n",
    "            \"params\": text_lora_parameters_one,\n",
    "            \"weight_decay\": args.adam_weight_decay_text_encoder,\n",
    "            \"lr\": args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,\n",
    "        }\n",
    "        text_lora_parameters_two_with_lr = {\n",
    "            \"params\": text_lora_parameters_two,\n",
    "            \"weight_decay\": args.adam_weight_decay_text_encoder,\n",
    "            \"lr\": args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,\n",
    "        }\n",
    "        params_to_optimize = [\n",
    "            unet_lora_parameters_with_lr,\n",
    "            text_lora_parameters_one_with_lr,\n",
    "            text_lora_parameters_two_with_lr,\n",
    "        ]\n",
    "    else:\n",
    "        params_to_optimize = [unet_lora_parameters_with_lr]\n",
    "\n",
    "    # Optimizer creation\n",
    "    if not (args.optimizer.lower() == \"prodigy\" or args.optimizer.lower() == \"adamw\"):\n",
    "        logger.warn(\n",
    "            f\"Unsupported choice of optimizer: {args.optimizer}.Supported optimizers include [adamW, prodigy].\"\n",
    "            \"Defaulting to adamW\"\n",
    "        )\n",
    "        args.optimizer = \"adamw\"\n",
    "\n",
    "    if args.use_8bit_adam and not args.optimizer.lower() == \"adamw\":\n",
    "        logger.warn(\n",
    "            f\"use_8bit_adam is ignored when optimizer is not set to 'AdamW'. Optimizer was \"\n",
    "            f\"set to {args.optimizer.lower()}\"\n",
    "        )\n",
    "\n",
    "    if args.optimizer.lower() == \"adamw\":\n",
    "        if args.use_8bit_adam:\n",
    "            try:\n",
    "                import bitsandbytes as bnb\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "                )\n",
    "\n",
    "            optimizer_class = bnb.optim.AdamW8bit\n",
    "        else:\n",
    "            optimizer_class = torch.optim.AdamW\n",
    "\n",
    "        optimizer = optimizer_class(\n",
    "            params_to_optimize,\n",
    "            betas=(args.adam_beta1, args.adam_beta2),\n",
    "            weight_decay=args.adam_weight_decay,\n",
    "            eps=args.adam_epsilon,\n",
    "        )\n",
    "\n",
    "    if args.optimizer.lower() == \"prodigy\":\n",
    "        try:\n",
    "            import prodigyopt\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`\"\n",
    "            )\n",
    "\n",
    "        optimizer_class = prodigyopt.Prodigy\n",
    "\n",
    "        if args.learning_rate <= 0.1:\n",
    "            logger.warn(\n",
    "                \"Learning rate is too low. When using prodigy, it's generally better to set learning rate around 1.0\"\n",
    "            )\n",
    "        if args.train_text_encoder and args.text_encoder_lr:\n",
    "            logger.warn(\n",
    "                f\"Learning rates were provided both for the unet and the text encoder- e.g. text_encoder_lr:\"\n",
    "                f\" {args.text_encoder_lr} and learning_rate: {args.learning_rate}. \"\n",
    "                f\"When using prodigy only learning_rate is used as the initial learning rate.\"\n",
    "            )\n",
    "            # changes the learning rate of text_encoder_parameters_one and text_encoder_parameters_two to be\n",
    "            # --learning_rate\n",
    "            params_to_optimize[1][\"lr\"] = args.learning_rate\n",
    "            params_to_optimize[2][\"lr\"] = args.learning_rate\n",
    "\n",
    "        optimizer = optimizer_class(\n",
    "            params_to_optimize,\n",
    "            lr=args.learning_rate,\n",
    "            betas=(args.adam_beta1, args.adam_beta2),\n",
    "            beta3=args.prodigy_beta3,\n",
    "            weight_decay=args.adam_weight_decay,\n",
    "            eps=args.adam_epsilon,\n",
    "            decouple=args.prodigy_decouple,\n",
    "            use_bias_correction=args.prodigy_use_bias_correction,\n",
    "            safeguard_warmup=args.prodigy_safeguard_warmup,\n",
    "        )\n",
    "\n",
    "    # Dataset and DataLoaders creation:\n",
    "    train_dataset = DreamBoothDataset(\n",
    "        instance_data_root=args.instance_data_dir,\n",
    "        instance_prompt=args.instance_prompt,\n",
    "        class_prompt=args.class_prompt,\n",
    "        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n",
    "        class_num=args.num_class_images,\n",
    "        size=args.resolution,\n",
    "        repeats=args.repeats,\n",
    "        center_crop=args.center_crop,\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    # Computes additional embeddings/ids required by the SDXL UNet.\n",
    "    # regular text embeddings (when `train_text_encoder` is not True)\n",
    "    # pooled text embeddings\n",
    "    # time ids\n",
    "\n",
    "    def compute_time_ids():\n",
    "        # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids\n",
    "        original_size = (args.resolution, args.resolution)\n",
    "        target_size = (args.resolution, args.resolution)\n",
    "        crops_coords_top_left = (\n",
    "            args.crops_coords_top_left_h,\n",
    "            args.crops_coords_top_left_w,\n",
    "        )\n",
    "        add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "        add_time_ids = torch.tensor([add_time_ids])\n",
    "        add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)\n",
    "        return add_time_ids\n",
    "\n",
    "    if not args.train_text_encoder:\n",
    "        tokenizers = [tokenizer_one, tokenizer_two]\n",
    "        text_encoders = [text_encoder_one, text_encoder_two]\n",
    "\n",
    "        def compute_text_embeddings(prompt, text_encoders, tokenizers):\n",
    "            with torch.no_grad():\n",
    "                prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                    text_encoders, tokenizers, prompt\n",
    "                )\n",
    "                prompt_embeds = prompt_embeds.to(accelerator.device)\n",
    "                pooled_prompt_embeds = pooled_prompt_embeds.to(accelerator.device)\n",
    "            return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "    # Handle instance prompt.\n",
    "    instance_time_ids = compute_time_ids()\n",
    "\n",
    "    # If no type of tuning is done on the text_encoder and custom instance prompts are NOT\n",
    "    # provided (i.e. the --instance_prompt is used for all images), we encode the instance prompt once to avoid\n",
    "    # the redundant encoding.\n",
    "    if not args.train_text_encoder and not train_dataset.custom_instance_prompts:\n",
    "        (\n",
    "            instance_prompt_hidden_states,\n",
    "            instance_pooled_prompt_embeds,\n",
    "        ) = compute_text_embeddings(args.instance_prompt, text_encoders, tokenizers)\n",
    "\n",
    "    # Handle class prompt for prior-preservation.\n",
    "    if args.with_prior_preservation:\n",
    "        class_time_ids = compute_time_ids()\n",
    "        if not args.train_text_encoder:\n",
    "            (\n",
    "                class_prompt_hidden_states,\n",
    "                class_pooled_prompt_embeds,\n",
    "            ) = compute_text_embeddings(args.class_prompt, text_encoders, tokenizers)\n",
    "\n",
    "    # Clear the memory here\n",
    "    if not args.train_text_encoder and not train_dataset.custom_instance_prompts:\n",
    "        del tokenizers, text_encoders\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # If custom instance prompts are NOT provided (i.e. the instance prompt is used for all images),\n",
    "    # pack the statically computed variables appropriately here. This is so that we don't\n",
    "    # have to pass them to the dataloader.\n",
    "    add_time_ids = instance_time_ids\n",
    "    if args.with_prior_preservation:\n",
    "        add_time_ids = torch.cat([add_time_ids, class_time_ids], dim=0)\n",
    "\n",
    "    if not train_dataset.custom_instance_prompts:\n",
    "        if not args.train_text_encoder:\n",
    "            prompt_embeds = instance_prompt_hidden_states\n",
    "            unet_add_text_embeds = instance_pooled_prompt_embeds\n",
    "            if args.with_prior_preservation:\n",
    "                prompt_embeds = torch.cat(\n",
    "                    [prompt_embeds, class_prompt_hidden_states], dim=0\n",
    "                )\n",
    "                unet_add_text_embeds = torch.cat(\n",
    "                    [unet_add_text_embeds, class_pooled_prompt_embeds], dim=0\n",
    "                )\n",
    "        # if we're optmizing the text encoder (both if instance prompt is used for all images or custom prompts) we need to tokenize and encode the\n",
    "        # batch prompts on all training steps\n",
    "        else:\n",
    "            tokens_one = tokenize_prompt(tokenizer_one, args.instance_prompt)\n",
    "            tokens_two = tokenize_prompt(tokenizer_two, args.instance_prompt)\n",
    "            if args.with_prior_preservation:\n",
    "                class_tokens_one = tokenize_prompt(tokenizer_one, args.class_prompt)\n",
    "                class_tokens_two = tokenize_prompt(tokenizer_two, args.class_prompt)\n",
    "                tokens_one = torch.cat([tokens_one, class_tokens_one], dim=0)\n",
    "                tokens_two = torch.cat([tokens_two, class_tokens_two], dim=0)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "        num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "        num_cycles=args.lr_num_cycles,\n",
    "        power=args.lr_power,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    if args.train_text_encoder:\n",
    "        (\n",
    "            unet,\n",
    "            text_encoder_one,\n",
    "            text_encoder_two,\n",
    "            optimizer,\n",
    "            train_dataloader,\n",
    "            lr_scheduler,\n",
    "        ) = accelerator.prepare(\n",
    "            unet,\n",
    "            text_encoder_one,\n",
    "            text_encoder_two,\n",
    "            optimizer,\n",
    "            train_dataloader,\n",
    "            lr_scheduler,\n",
    "        )\n",
    "    else:\n",
    "        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            unet, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"dreambooth-lora-sd-xl\", config=vars(args))\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    "    )\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the mos recent checkpoint\n",
    "            dirs = os.listdir(args.output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "        if path is None:\n",
    "            accelerator.print(\n",
    "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "            )\n",
    "            args.resume_from_checkpoint = None\n",
    "            initial_global_step = 0\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "            initial_global_step = global_step\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "\n",
    "    else:\n",
    "        initial_global_step = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(0, args.max_train_steps),\n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "    for epoch in range(first_epoch, args.num_train_epochs):\n",
    "        unet.train()\n",
    "        if args.train_text_encoder:\n",
    "            text_encoder_one.train()\n",
    "            text_encoder_two.train()\n",
    "\n",
    "            # set top parameter requires_grad = True for gradient checkpointing works\n",
    "            text_encoder_one.text_model.embeddings.requires_grad_(True)\n",
    "            text_encoder_two.text_model.embeddings.requires_grad_(True)\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                pixel_values = batch[\"pixel_values\"].to(dtype=vae.dtype)\n",
    "                prompts = batch[\"prompts\"]\n",
    "\n",
    "                # encode batch prompts when custom prompts are provided for each image -\n",
    "                if train_dataset.custom_instance_prompts:\n",
    "                    if not args.train_text_encoder:\n",
    "                        prompt_embeds, unet_add_text_embeds = compute_text_embeddings(\n",
    "                            prompts, text_encoders, tokenizers\n",
    "                        )\n",
    "                    else:\n",
    "                        tokens_one = tokenize_prompt(tokenizer_one, prompts)\n",
    "                        tokens_two = tokenize_prompt(tokenizer_two, prompts)\n",
    "\n",
    "                # Convert images to latent space\n",
    "                model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "                model_input = model_input * vae.config.scaling_factor\n",
    "                if args.pretrained_vae_model_name_or_path is None:\n",
    "                    model_input = model_input.to(weight_dtype)\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(model_input)\n",
    "                bsz = model_input.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=model_input.device,\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the model input according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_model_input = noise_scheduler.add_noise(\n",
    "                    model_input, noise, timesteps\n",
    "                )\n",
    "\n",
    "                # Calculate the elements to repeat depending on the use of prior-preservation and custom captions.\n",
    "                if not train_dataset.custom_instance_prompts:\n",
    "                    elems_to_repeat_text_embeds = (\n",
    "                        bsz // 2 if args.with_prior_preservation else bsz\n",
    "                    )\n",
    "                    elems_to_repeat_time_ids = (\n",
    "                        bsz // 2 if args.with_prior_preservation else bsz\n",
    "                    )\n",
    "                else:\n",
    "                    elems_to_repeat_text_embeds = 1\n",
    "                    elems_to_repeat_time_ids = (\n",
    "                        bsz // 2 if args.with_prior_preservation else bsz\n",
    "                    )\n",
    "\n",
    "                # Predict the noise residual\n",
    "                if not args.train_text_encoder:\n",
    "                    unet_added_conditions = {\n",
    "                        \"time_ids\": add_time_ids.repeat(elems_to_repeat_time_ids, 1),\n",
    "                        \"text_embeds\": unet_add_text_embeds.repeat(\n",
    "                            elems_to_repeat_text_embeds, 1\n",
    "                        ),\n",
    "                    }\n",
    "                    prompt_embeds_input = prompt_embeds.repeat(\n",
    "                        elems_to_repeat_text_embeds, 1, 1\n",
    "                    )\n",
    "                    model_pred = unet(\n",
    "                        noisy_model_input,\n",
    "                        timesteps,\n",
    "                        prompt_embeds_input,\n",
    "                        added_cond_kwargs=unet_added_conditions,\n",
    "                    ).sample\n",
    "                else:\n",
    "                    unet_added_conditions = {\n",
    "                        \"time_ids\": add_time_ids.repeat(elems_to_repeat_time_ids, 1)\n",
    "                    }\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one, text_encoder_two],\n",
    "                        tokenizers=None,\n",
    "                        prompt=None,\n",
    "                        text_input_ids_list=[tokens_one, tokens_two],\n",
    "                    )\n",
    "                    unet_added_conditions.update(\n",
    "                        {\n",
    "                            \"text_embeds\": pooled_prompt_embeds.repeat(\n",
    "                                elems_to_repeat_text_embeds, 1\n",
    "                            )\n",
    "                        }\n",
    "                    )\n",
    "                    prompt_embeds_input = prompt_embeds.repeat(\n",
    "                        elems_to_repeat_text_embeds, 1, 1\n",
    "                    )\n",
    "                    model_pred = unet(\n",
    "                        noisy_model_input,\n",
    "                        timesteps,\n",
    "                        prompt_embeds_input,\n",
    "                        added_cond_kwargs=unet_added_conditions,\n",
    "                    ).sample\n",
    "\n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                    )\n",
    "\n",
    "                if args.with_prior_preservation:\n",
    "                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n",
    "                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "                    # Compute prior loss\n",
    "                    prior_loss = F.mse_loss(\n",
    "                        model_pred_prior.float(), target_prior.float(), reduction=\"mean\"\n",
    "                    )\n",
    "\n",
    "                if args.snr_gamma is None:\n",
    "                    loss = F.mse_loss(\n",
    "                        model_pred.float(), target.float(), reduction=\"mean\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
    "                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
    "                    # This is discussed in Section 4.2 of the same paper.\n",
    "                    snr = compute_snr(noise_scheduler, timesteps)\n",
    "                    base_weight = (\n",
    "                        torch.stack(\n",
    "                            [snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1\n",
    "                        ).min(dim=1)[0]\n",
    "                        / snr\n",
    "                    )\n",
    "\n",
    "                    if noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                        # Velocity objective needs to be floored to an SNR weight of one.\n",
    "                        mse_loss_weights = base_weight + 1\n",
    "                    else:\n",
    "                        # Epsilon and sample both use the same loss weights.\n",
    "                        mse_loss_weights = base_weight\n",
    "\n",
    "                    loss = F.mse_loss(\n",
    "                        model_pred.float(), target.float(), reduction=\"none\"\n",
    "                    )\n",
    "                    loss = (\n",
    "                        loss.mean(dim=list(range(1, len(loss.shape))))\n",
    "                        * mse_loss_weights\n",
    "                    )\n",
    "                    loss = loss.mean()\n",
    "\n",
    "                if args.with_prior_preservation:\n",
    "                    # Add the prior loss to the instance loss.\n",
    "                    loss = loss + args.prior_loss_weight * prior_loss\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = (\n",
    "                        itertools.chain(\n",
    "                            unet_lora_parameters,\n",
    "                            text_lora_parameters_one,\n",
    "                            text_lora_parameters_two,\n",
    "                        )\n",
    "                        if args.train_text_encoder\n",
    "                        else unet_lora_parameters\n",
    "                    )\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if global_step % args.checkpointing_steps == 0:\n",
    "                        # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                        if args.checkpoints_total_limit is not None:\n",
    "                            checkpoints = os.listdir(args.output_dir)\n",
    "                            checkpoints = [\n",
    "                                d for d in checkpoints if d.startswith(\"checkpoint\")\n",
    "                            ]\n",
    "                            checkpoints = sorted(\n",
    "                                checkpoints, key=lambda x: int(x.split(\"-\")[1])\n",
    "                            )\n",
    "\n",
    "                            # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                            if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                                num_to_remove = (\n",
    "                                    len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                                )\n",
    "                                removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                                logger.info(\n",
    "                                    f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                                )\n",
    "                                logger.info(\n",
    "                                    f\"removing checkpoints: {', '.join(removing_checkpoints)}\"\n",
    "                                )\n",
    "\n",
    "                                for removing_checkpoint in removing_checkpoints:\n",
    "                                    removing_checkpoint = os.path.join(\n",
    "                                        args.output_dir, removing_checkpoint\n",
    "                                    )\n",
    "                                    shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                        save_path = os.path.join(\n",
    "                            args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                        )\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            if (\n",
    "                args.validation_prompt is not None\n",
    "                and epoch % args.validation_epochs == 0\n",
    "            ):\n",
    "                logger.info(\n",
    "                    f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "                    f\" {args.validation_prompt}.\"\n",
    "                )\n",
    "                # create pipeline\n",
    "                if not args.train_text_encoder:\n",
    "                    text_encoder_one = text_encoder_cls_one.from_pretrained(\n",
    "                        args.pretrained_model_name_or_path,\n",
    "                        subfolder=\"text_encoder\",\n",
    "                        revision=args.revision,\n",
    "                    )\n",
    "                    text_encoder_two = text_encoder_cls_two.from_pretrained(\n",
    "                        args.pretrained_model_name_or_path,\n",
    "                        subfolder=\"text_encoder_2\",\n",
    "                        revision=args.revision,\n",
    "                    )\n",
    "                pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "                    args.pretrained_model_name_or_path,\n",
    "                    vae=vae,\n",
    "                    text_encoder=accelerator.unwrap_model(text_encoder_one),\n",
    "                    text_encoder_2=accelerator.unwrap_model(text_encoder_two),\n",
    "                    unet=accelerator.unwrap_model(unet),\n",
    "                    revision=args.revision,\n",
    "                    torch_dtype=weight_dtype,\n",
    "                )\n",
    "\n",
    "                # We train on the simplified learning objective. If we were previously predicting a variance, we need the scheduler to ignore it\n",
    "                scheduler_args = {}\n",
    "\n",
    "                if \"variance_type\" in pipeline.scheduler.config:\n",
    "                    variance_type = pipeline.scheduler.config.variance_type\n",
    "\n",
    "                    if variance_type in [\"learned\", \"learned_range\"]:\n",
    "                        variance_type = \"fixed_small\"\n",
    "\n",
    "                    scheduler_args[\"variance_type\"] = variance_type\n",
    "\n",
    "                pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                    pipeline.scheduler.config, **scheduler_args\n",
    "                )\n",
    "\n",
    "                pipeline = pipeline.to(accelerator.device)\n",
    "                pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "                # run inference\n",
    "                generator = (\n",
    "                    torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "                    if args.seed\n",
    "                    else None\n",
    "                )\n",
    "                pipeline_args = {\"prompt\": args.validation_prompt}\n",
    "\n",
    "                images = [\n",
    "                    pipeline(**pipeline_args, generator=generator).images[0]\n",
    "                    for _ in range(args.num_validation_images)\n",
    "                ]\n",
    "\n",
    "                for tracker in accelerator.trackers:\n",
    "                    if tracker.name == \"tensorboard\":\n",
    "                        np_images = np.stack([np.asarray(img) for img in images])\n",
    "                        tracker.writer.add_images(\n",
    "                            \"validation\", np_images, epoch, dataformats=\"NHWC\"\n",
    "                        )\n",
    "                    if tracker.name == \"wandb\":\n",
    "                        tracker.log(\n",
    "                            {\n",
    "                                \"validation\": [\n",
    "                                    wandb.Image(\n",
    "                                        image, caption=f\"{i}: {args.validation_prompt}\"\n",
    "                                    )\n",
    "                                    for i, image in enumerate(images)\n",
    "                                ]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                del pipeline\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the lora layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = accelerator.unwrap_model(unet)\n",
    "        unet = unet.to(torch.float32)\n",
    "        unet_lora_layers = unet_lora_state_dict(unet)\n",
    "\n",
    "        if args.train_text_encoder:\n",
    "            text_encoder_one = accelerator.unwrap_model(text_encoder_one)\n",
    "            text_encoder_lora_layers = text_encoder_lora_state_dict(\n",
    "                text_encoder_one.to(torch.float32)\n",
    "            )\n",
    "            text_encoder_two = accelerator.unwrap_model(text_encoder_two)\n",
    "            text_encoder_2_lora_layers = text_encoder_lora_state_dict(\n",
    "                text_encoder_two.to(torch.float32)\n",
    "            )\n",
    "        else:\n",
    "            text_encoder_lora_layers = None\n",
    "            text_encoder_2_lora_layers = None\n",
    "\n",
    "        StableDiffusionXLPipeline.save_lora_weights(\n",
    "            save_directory=args.output_dir,\n",
    "            unet_lora_layers=unet_lora_layers,\n",
    "            text_encoder_lora_layers=text_encoder_lora_layers,\n",
    "            text_encoder_2_lora_layers=text_encoder_2_lora_layers,\n",
    "        )\n",
    "        \n",
    "        # remove unuse models for save GPU memory\n",
    "        unet = unet.cpu()\n",
    "        text_encoder_one = text_encoder_one.cpu()\n",
    "        text_encoder_two = text_encoder_two.cpu()\n",
    "        del unet, text_encoder_one, text_encoder_two\n",
    "        del optimizer\n",
    "        if args.train_text_encoder:\n",
    "            del text_encoder_lora_layers, text_encoder_2_lora_layers\n",
    "\n",
    "        # Final inference\n",
    "        # Load previous pipeline\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            vae_path,\n",
    "            subfolder=\"vae\" if args.pretrained_vae_model_name_or_path is None else None,\n",
    "            revision=args.revision,\n",
    "            torch_dtype=weight_dtype,\n",
    "        )\n",
    "        pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            vae=vae,\n",
    "            revision=args.revision,\n",
    "            torch_dtype=weight_dtype,\n",
    "        )\n",
    "\n",
    "        # We train on the simplified learning objective. If we were previously predicting a variance, we need the scheduler to ignore it\n",
    "        scheduler_args = {}\n",
    "\n",
    "        if \"variance_type\" in pipeline.scheduler.config:\n",
    "            variance_type = pipeline.scheduler.config.variance_type\n",
    "\n",
    "            if variance_type in [\"learned\", \"learned_range\"]:\n",
    "                variance_type = \"fixed_small\"\n",
    "\n",
    "            scheduler_args[\"variance_type\"] = variance_type\n",
    "\n",
    "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            pipeline.scheduler.config, **scheduler_args\n",
    "        )\n",
    "\n",
    "        # load attention processors\n",
    "        pipeline.load_lora_weights(args.output_dir)\n",
    "\n",
    "        # run inference\n",
    "        images = []\n",
    "        if args.validation_prompt and args.num_validation_images > 0:\n",
    "            pipeline = pipeline.to(accelerator.device)\n",
    "            generator = (\n",
    "                torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "                if args.seed\n",
    "                else None\n",
    "            )\n",
    "            images = [\n",
    "                pipeline(\n",
    "                    args.validation_prompt, num_inference_steps=25, generator=generator\n",
    "                ).images[0]\n",
    "                for _ in range(args.num_validation_images)\n",
    "            ]\n",
    "\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\n",
    "                        \"test\", np_images, epoch, dataformats=\"NHWC\"\n",
    "                    )\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"test\": [\n",
    "                                wandb.Image(\n",
    "                                    image, caption=f\"{i}: {args.validation_prompt}\"\n",
    "                                )\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if args.push_to_hub:\n",
    "            save_model_card(\n",
    "                repo_id,\n",
    "                images=images,\n",
    "                base_model=args.pretrained_model_name_or_path,\n",
    "                train_text_encoder=args.train_text_encoder,\n",
    "                instance_prompt=args.instance_prompt,\n",
    "                validation_prompt=args.validation_prompt,\n",
    "                repo_folder=args.output_dir,\n",
    "                vae_path=args.pretrained_vae_model_name_or_path,\n",
    "            )\n",
    "            upload_folder(\n",
    "                repo_id=repo_id,\n",
    "                folder_path=args.output_dir,\n",
    "                commit_message=\"End of training\",\n",
    "                ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "            )\n",
    "\n",
    "    accelerator.end_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
